from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required, user_passes_test
from django.http import JsonResponse, HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib import messages
from django.db.models import Q, Count, Avg
from django.core.paginator import Paginator
from django.utils import timezone
from datetime import datetime, timedelta
import json
import hashlib
import numpy as np
import pydicom
import os
import threading
import time
import re
import requests
try:
    import onnxruntime as ort
except Exception:
    ort = None
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except Exception:
    AutoTokenizer = None
    AutoModelForSequenceClassification = None

from worklist.models import Study, DicomImage, Series
from accounts.models import User
from .models import (
    AIModel, AIAnalysis, AutoReportTemplate, AutoGeneratedReport,
    AITrainingData, AIPerformanceMetric, AIFeedback
)
from .inference import ModelRegistry
from .utils import (
    MEDICAL_BOOK_REFERENCES, _get_online_references, _normalize_abnormality_label,
    _compute_ai_triage, _apply_ai_triage, simulate_ai_analysis, _upgrade_study_priority, _notify_ai_triage,
    run_full_series_inference
)
from .tasks import run_ai_analysis
from .reporting import persist_report_on_analysis, generate_report_content
def is_admin_or_radiologist(user):
    """Check if user is admin or radiologist"""
    return user.is_authenticated and (user.is_admin() or user.is_radiologist())

@login_required
def ai_dashboard(request):
    """AI analysis dashboard with comprehensive overview"""
    user = request.user
    
    # Get AI models statistics
    total_models = AIModel.objects.filter(is_active=True).count()
    active_analyses = AIAnalysis.objects.filter(status__in=['pending', 'processing']).count()
    completed_today = AIAnalysis.objects.filter(
        completed_at__date=timezone.now().date(),
        status='completed'
    ).count()
    total_analyses = AIAnalysis.objects.count()
    processing_queue = active_analyses

    # Average accuracy from available performance metrics (percentage)
    avg_accuracy = (
        AIPerformanceMetric.objects.filter(ai_model__is_active=True)
        .aggregate(avg=Avg('accuracy'))
        .get('avg')
    )
    accuracy_rate = round((avg_accuracy or 0) * 100, 1)
    
    # Get recent analyses
    if user.is_facility_user():
        recent_analyses = AIAnalysis.objects.filter(
            study__facility=user.facility
        ).select_related('study', 'ai_model').order_by('-requested_at')[:10]
    else:
        recent_analyses = AIAnalysis.objects.select_related(
            'study', 'ai_model'
        ).order_by('-requested_at')[:10]
    
    # Get pending auto-reports
    if user.is_radiologist() or user.is_admin():
        pending_reports = AutoGeneratedReport.objects.filter(
            review_status='pending'
        ).select_related('study', 'ai_analysis').order_by('-generated_at')[:5]
    else:
        pending_reports = []
    
    # Get model performance summary
    model_performance = []
    for model in AIModel.objects.filter(is_active=True)[:5]:
        latest_metric = model.performance_metrics.first()
        model_performance.append({
            'model': model,
            'accuracy': latest_metric.accuracy if latest_metric else 0,
            'total_analyses': model.total_analyses,
            'avg_time': model.avg_processing_time
        })
    
    context = {
        # Template-friendly dashboard stats
        'total_analyses': total_analyses,
        'active_models': total_models,
        'processing_queue': processing_queue,
        'accuracy_rate': accuracy_rate,

        'total_models': total_models,
        'active_analyses': active_analyses,
        'completed_today': completed_today,
        'recent_analyses': recent_analyses,
        'pending_reports': pending_reports,
        'model_performance': model_performance,
        'user': user,
    }
    
    return render(request, 'ai_analysis/dashboard.html', context)


@login_required
@user_passes_test(is_admin_or_radiologist)
def study_picker(request):
    """Pick a study to run AI analysis on."""
    user = request.user

    studies = Study.objects.select_related('patient', 'modality', 'facility').order_by('-upload_date')

    # Facility scoping
    if user.is_facility_user():
        studies = studies.filter(facility=user.facility)

    q = (request.GET.get('q') or '').strip()
    if q:
        studies = studies.filter(
            Q(accession_number__icontains=q)
            | Q(patient__first_name__icontains=q)
            | Q(patient__last_name__icontains=q)
            | Q(patient__patient_id__icontains=q)
        )

    paginator = Paginator(studies, 25)
    page_number = request.GET.get('page')
    studies_page = paginator.get_page(page_number)

    return render(
        request,
        'ai_analysis/study_picker.html',
        {
            'studies': studies_page,
            'query': q,
        },
    )

@login_required
@csrf_exempt
def analyze_study(request, study_id):
    """Run AI analysis on study"""
    study = get_object_or_404(Study, id=study_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        # Only administrators or radiologists can initiate new AI analyses
        if not is_admin_or_radiologist(user):
            return JsonResponse({'error': 'Only administrators or radiologists can start AI analyses'}, status=403)
        try:
            # Get selected AI models
            model_ids = request.POST.getlist('ai_models')
            priority = request.POST.get('priority', 'normal')
            
            if not model_ids:
                return JsonResponse({'error': 'Please select at least one AI model'}, status=400)
            
            # Create analyses for each selected model
            analyses = []
            for model_id in model_ids:
                ai_model = get_object_or_404(AIModel, id=model_id, is_active=True)
                
                # Check if analysis already exists
                existing = AIAnalysis.objects.filter(
                    study=study,
                    ai_model=ai_model,
                    status__in=['pending', 'processing', 'completed']
                ).first()
                
                if existing:
                    continue
                
                # Create new analysis
                analysis = AIAnalysis.objects.create(
                    study=study,
                    ai_model=ai_model,
                    priority=priority,
                    status='pending'
                )
                analyses.append(analysis)
            
            # Start processing in background
            if analyses:
                for analysis in analyses:
                    run_ai_analysis.delay(analysis.id)
            
            return JsonResponse({
                'success': True,
                'message': f'Started AI analysis with {len(analyses)} models',
                'analysis_ids': [a.id for a in analyses]
            })
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    # GET request - show analysis form
    available_models = []
    if is_admin_or_radiologist(user):
        # Filter models
        available_models = AIModel.objects.filter(
            is_active=True,
            modality__in=[study.modality.code, 'ALL']
        )
        
        # Enforce Subscription
        # If user is admin/radiologist, check their facility subscription if applicable.
        # If no facility linked (e.g. platform admin), allow access.
        if user.facility and not user.facility.has_ai_subscription:
             # Hide models that require subscription
             available_models = available_models.filter(requires_subscription=False)
        
        # If subscription expired, treat as no subscription
        if user.facility and user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now():
             available_models = available_models.filter(requires_subscription=False)

    # Get existing analyses
    existing_analyses = AIAnalysis.objects.filter(
        study=study
    ).select_related('ai_model').order_by('-requested_at')
    
    context = {
        'study': study,
        'available_models': available_models,
        'existing_analyses': existing_analyses,
    }
    
    return render(request, 'ai_analysis/analyze_study.html', context)

@login_required
@csrf_exempt
def api_analysis_status(request, analysis_id):
    """Get analysis status and progress"""
    analysis = get_object_or_404(AIAnalysis, id=analysis_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and analysis.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    progress_percentage = 0
    if analysis.status == 'completed':
        progress_percentage = 100
    elif analysis.status == 'processing':
        # Estimate progress based on processing time
        if analysis.started_at:
            elapsed = (timezone.now() - analysis.started_at).total_seconds()
            estimated_total = analysis.ai_model.avg_processing_time or 60
            progress_percentage = min(90, (elapsed / estimated_total) * 100)
    
    # Only administrators/radiologists get full detailed analysis
    clinician = is_admin_or_radiologist(user)
    data = {
        'id': analysis.id,
        'status': analysis.status,
        'progress_percentage': round(progress_percentage, 2),
        'confidence_score': analysis.confidence_score,
        'requested_at': analysis.requested_at.isoformat(),
        'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None,
        'ai_model': {
            'name': analysis.ai_model.name,
            'version': analysis.ai_model.version,
            'type': analysis.ai_model.model_type
        }
    }
    if clinician:
        data.update({
            'findings': analysis.findings,
            'abnormalities_detected': analysis.abnormalities_detected,
            'measurements': analysis.measurements,
            'overlays': analysis.measurements.get('overlays', []), # Add overlays support
            'processing_time': analysis.processing_time,
            'error_message': analysis.error_message,
        })
        # Final report payload (primary UI output when completed).
        if analysis.status == 'completed':
            report = (analysis.measurements or {}).get('report') or {}
            data['report'] = report
            data['report_text'] = report.get('text', '')
                
    else:
        # Minimal, non-intrusive preliminary summary for non-clinician roles
        data.update({
            'summary': 'Preliminary AI review complete' if analysis.status == 'completed' else 'AI review in progress',
        })
    
    return JsonResponse(data)


@login_required
@csrf_exempt
def api_analyze_series(request, series_id):
    """
    DICOM viewer integration endpoint.
    Runs a quick (synchronous) AI analysis for a given Series and returns a simple
    findings list for UI display.
    """
    if request.method != 'POST':
        return JsonResponse({'success': False, 'error': 'Method not allowed'}, status=405)

    user = request.user
    series = get_object_or_404(Series, id=series_id)
    study = series.study

    # Facility permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'success': False, 'error': 'Permission denied'}, status=403)

    # Permission to initiate AI runs:
    # - admins/radiologists always
    # - facility users only when their facility has an active AI subscription
    can_start_ai = False
    if is_admin_or_radiologist(user):
        can_start_ai = True
    elif user.is_facility_user() and getattr(user, 'facility', None):
        has_sub = bool(user.facility.has_ai_subscription)
        is_expired = bool(user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now())
        can_start_ai = has_sub and not is_expired

    if not can_start_ai:
        return JsonResponse(
            {'success': False, 'error': 'AI access requires a radiologist/admin account or an active facility AI subscription'},
            status=403,
        )

    # Subscription Check for Quick AI
    if user.facility:
        has_sub = user.facility.has_ai_subscription
        is_expired = user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now()
        if not has_sub or is_expired:
             # Only allow free models if any exists, but for "Quick AI" we typically want the best one.
             # We will check the selected model later, but if they have NO subscription, we might block early or limit choices.
             pass

    # Pick one suitable active model for this modality
    modality_code = getattr(study.modality, 'code', None)
    models_query = AIModel.objects.filter(is_active=True, modality__in=[modality_code, 'ALL'])
    
    # Enforce subscription filter on model selection
    if user.facility and (not user.facility.has_ai_subscription or (user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now())):
        models_query = models_query.filter(requires_subscription=False)

    # Allow model selection via POST
    requested_model_id = request.POST.get('model_id')
    ai_model = None
    if requested_model_id:
        try:
            ai_model = models_query.get(id=requested_model_id)
        except AIModel.DoesNotExist:
            pass # Fallback to auto-selection if invalid/unauthorized
    
    if not ai_model:
        ai_model = models_query.order_by('model_type', '-created_at').first()
    
    if not ai_model:
        if user.facility and not user.facility.has_ai_subscription:
             return JsonResponse({'success': False, 'error': 'AI Subscription required for this analysis.'}, status=403)
        
        return JsonResponse(
            {'success': False, 'error': f'No active AI models available for modality {modality_code}'},
            status=400,
        )

    # Create and run analysis synchronously (keeps the viewer UX simple)
    analysis = AIAnalysis.objects.create(
        study=study,
        ai_model=ai_model,
        priority='normal',
        status='pending',
    )

    try:
        analysis.start_processing()
        
        # Use real inference engine (via registry) with fallback to simulation
        model_adapter = ModelRegistry.get_model(ai_model)

        # Full-series analysis (representative sampling across the series)
        try:
            max_slices = int(request.POST.get('max_slices') or 24)
        except Exception:
            max_slices = 24
        max_slices = max(1, min(64, max_slices))

        if series.images.exists():
            try:
                results = run_full_series_inference(model_adapter, series.images.all(), max_slices=max_slices)
                results.setdefault('measurements', {})
                if isinstance(results['measurements'], dict):
                    results['measurements']['series_id'] = int(series.id)
                    results['measurements']['study_id'] = int(study.id)
                    results['measurements']['max_slices'] = int(max_slices)
            except Exception:
                results = simulate_ai_analysis(analysis)
        else:
            results = simulate_ai_analysis(analysis)
            
        analysis.complete_analysis(results)
        try:
            _apply_ai_triage(analysis)
        except Exception:
            pass
        try:
            analysis.refresh_from_db()
            persist_report_on_analysis(analysis)
        except Exception:
            pass

        conf = float(analysis.confidence_score or 0.0)

        findings_list = []
        if analysis.findings:
            findings_list.append(
                {
                    'type': ai_model.name,
                    'description': analysis.findings,
                    'confidence': conf,
                }
            )
        for abn in (analysis.abnormalities_detected or []):
            label = _normalize_abnormality_label(abn)
            if label:
                findings_list.append(
                    {
                        'type': 'Abnormality',
                        'description': label,
                        'confidence': conf,
                    }
                )

        # Handle online references for Quick AI too
        online_refs = analysis.measurements.get('online_references') or []
        if not online_refs:
            # Try to fetch them if we have findings but no cached refs
            keywords = []
            abnormalities = analysis.abnormalities_detected or []
            for a in abnormalities:
                label = _normalize_abnormality_label(a).lower()
                if label:
                    clean = re.sub(r'suspicion|possible|probable', '', label).strip()
                    if clean: keywords.append(clean)
            
            if keywords:
                online_refs = _get_online_references(keywords)
                if online_refs:
                    analysis.measurements['online_references'] = online_refs
                    analysis.save(update_fields=['measurements'])

        m_out = analysis.measurements or {}
        if not isinstance(m_out, dict):
            m_out = {}
        return JsonResponse(
            {
                'success': True,
                'analysis_id': analysis.id,
                'study_id': study.id,
                'series_id': series.id,
                'findings': findings_list,
                'annotations': [],
                'measurements': m_out,
                # overlays are per-slice; each item should include image_id/instance_number when available
                'overlays': m_out.get('overlays', []),
                'online_references': online_refs,
                'report': m_out.get('report', {}),
                'report_text': (m_out.get('report') or {}).get('text', ''),
                'triage': {
                    'triage_level': m_out.get('triage_level'),
                    'triage_score': m_out.get('triage_score'),
                    'flagged': bool(m_out.get('triage_flagged')),
                },
            }
        )
    except Exception as e:
        analysis.status = 'failed'
        analysis.error_message = str(e)
        analysis.save(update_fields=['status', 'error_message'])
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@login_required
@csrf_exempt
def api_analyze_study(request, study_id):
    """
    Study-level AI analysis endpoint.
    Picks the largest series in the study and runs full-series inference, then applies triage + report.
    Designed for Worklist + Reporting UI integration.
    """
    if request.method != 'POST':
        return JsonResponse({'success': False, 'error': 'Method not allowed'}, status=405)

    user = request.user
    study = get_object_or_404(Study, id=study_id)

    # Facility permissions
    if user.is_facility_user() and getattr(user, 'facility', None) and study.facility != user.facility:
        return JsonResponse({'success': False, 'error': 'Permission denied'}, status=403)

    # Permission to initiate AI runs:
    can_start_ai = False
    if is_admin_or_radiologist(user):
        can_start_ai = True
    elif user.is_facility_user() and getattr(user, 'facility', None):
        has_sub = bool(user.facility.has_ai_subscription)
        is_expired = bool(user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now())
        can_start_ai = has_sub and not is_expired

    if not can_start_ai:
        return JsonResponse(
            {'success': False, 'error': 'AI access requires a radiologist/admin account or an active facility AI subscription'},
            status=403,
        )

    # Pick an active model for this modality (same selection rules as series endpoint)
    modality_code = getattr(study.modality, 'code', None)
    models_query = AIModel.objects.filter(is_active=True, modality__in=[modality_code, 'ALL'])
    if user.facility and (not user.facility.has_ai_subscription or (user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now())):
        models_query = models_query.filter(requires_subscription=False)

    requested_model_id = request.POST.get('model_id')
    ai_model = None
    if requested_model_id:
        try:
            ai_model = models_query.get(id=requested_model_id)
        except AIModel.DoesNotExist:
            ai_model = None
    if not ai_model:
        ai_model = models_query.order_by('model_type', '-created_at').first()
    if not ai_model:
        return JsonResponse({'success': False, 'error': f'No active AI models available for modality {modality_code}'}, status=400)

    # Find the target series (largest stack)
    series_qs = study.series_set.all()
    target_series = None
    try:
        target_series = max(series_qs, key=lambda s: s.images.count()) if series_qs else None
    except Exception:
        target_series = series_qs.first() if series_qs else None
    if not target_series:
        return JsonResponse({'success': False, 'error': 'No series available for study'}, status=400)

    try:
        max_slices = int(request.POST.get('max_slices') or 24)
    except Exception:
        max_slices = 24
    max_slices = max(1, min(64, max_slices))

    analysis = AIAnalysis.objects.create(
        study=study,
        ai_model=ai_model,
        priority='normal',
        status='pending',
    )

    try:
        analysis.start_processing()
        model_adapter = ModelRegistry.get_model(ai_model)
        if target_series.images.exists():
            results = run_full_series_inference(model_adapter, target_series.images.all(), max_slices=max_slices)
            results.setdefault('measurements', {})
            if isinstance(results['measurements'], dict):
                results['measurements']['series_id'] = int(target_series.id)
                results['measurements']['study_id'] = int(study.id)
                results['measurements']['max_slices'] = int(max_slices)
        else:
            results = simulate_ai_analysis(analysis)

        analysis.complete_analysis(results)
        try:
            _apply_ai_triage(analysis)
        except Exception:
            pass
        try:
            analysis.refresh_from_db()
            persist_report_on_analysis(analysis)
        except Exception:
            pass

        # Handle online references for Study-level Quick AI too
        online_refs = (analysis.measurements or {}).get('online_references') or []
        if not online_refs:
            keywords = []
            abnormalities = analysis.abnormalities_detected or []
            for a in abnormalities:
                label = _normalize_abnormality_label(a).lower()
                if label:
                    clean = re.sub(r'suspicion|possible|probable', '', label).strip()
                    if clean:
                        keywords.append(clean)
            if keywords:
                online_refs = _get_online_references(keywords)
                if online_refs:
                    try:
                        mtmp = analysis.measurements or {}
                        if not isinstance(mtmp, dict):
                            mtmp = {}
                        mtmp['online_references'] = online_refs
                        analysis.measurements = mtmp
                        analysis.save(update_fields=['measurements'])
                    except Exception:
                        pass

        m_out = analysis.measurements or {}
        if not isinstance(m_out, dict):
            m_out = {}
        return JsonResponse({
            'success': True,
            'analysis_id': analysis.id,
            'study_id': study.id,
            'series_id': int(target_series.id),
            'measurements': m_out,
            'overlays': m_out.get('overlays', []),
            'online_references': online_refs,
            'report': m_out.get('report', {}),
            'report_text': (m_out.get('report') or {}).get('text', ''),
            'triage': {
                'triage_level': m_out.get('triage_level'),
                'triage_score': m_out.get('triage_score'),
                'flagged': bool(m_out.get('triage_flagged')),
            },
        })
    except Exception as e:
        analysis.status = 'failed'
        analysis.error_message = str(e)
        analysis.save(update_fields=['status', 'error_message'])
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

@login_required
def api_list_models(request):
    """List available AI models for the viewer"""
    user = request.user
    if not is_admin_or_radiologist(user):
        return JsonResponse({'models': []})
        
    modality = request.GET.get('modality')
    
    models_query = AIModel.objects.filter(is_active=True)
    if modality:
        models_query = models_query.filter(modality__in=[modality, 'ALL'])
        
    # Enforce subscription
    if user.facility:
        is_sub_valid = user.facility.has_ai_subscription
        if user.facility.subscription_expires_at and user.facility.subscription_expires_at < timezone.now():
            is_sub_valid = False
            
        if not is_sub_valid:
            models_query = models_query.filter(requires_subscription=False)
            
    models_data = []
    for m in models_query:
        models_data.append({
            'id': m.id,
            'name': m.name,
            'type': m.model_type,
            'modality': m.modality,
            'description': m.description,
            'requires_subscription': m.requires_subscription
        })
        
    return JsonResponse({'models': models_data})

@login_required
@user_passes_test(is_admin_or_radiologist)
@csrf_exempt
def generate_auto_report(request, study_id):
    """Generate automatic report from AI analysis"""
    study = get_object_or_404(Study, id=study_id)
    user = request.user
    
    # Check facility permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    try:
        # Get completed AI analyses for this study
        analyses = AIAnalysis.objects.filter(
            study=study,
            status='completed'
        ).select_related('ai_model')
        
        if not analyses:
            return JsonResponse({'error': 'No completed AI analyses found for this study'}, status=400)
        
        # Find appropriate template
        template = AutoReportTemplate.objects.filter(
            modality=study.modality.code,
            body_part__icontains=study.body_part,
            is_active=True
        ).first()
        
        if not template:
            # Use generic template
            template = AutoReportTemplate.objects.filter(
                modality=study.modality.code,
                is_active=True
            ).first()
        
        if not template:
            return JsonResponse({'error': 'No suitable report template found'}, status=400)
        
        # Generate report content
        report_data = generate_report_content(study, analyses, template)
        
        # Create auto-generated report
        auto_report = AutoGeneratedReport.objects.create(
            study=study,
            template=template,
            ai_analysis=analyses.first(),  # Primary analysis
            generated_findings=report_data['findings'],
            generated_impression=report_data['impression'],
            generated_recommendations=report_data['recommendations'],
            overall_confidence=report_data['confidence'],
            requires_review=report_data['confidence'] < template.confidence_threshold
        )

        # Return references + triage metadata from the primary analysis (if present)
        primary = analyses.first()
        m = (getattr(primary, 'measurements', {}) or {})
        if not isinstance(m, dict):
            m = {}
        
        return JsonResponse({
            'success': True,
            'report_id': auto_report.id,
            'findings': auto_report.generated_findings,
            'impression': auto_report.generated_impression,
            'recommendations': auto_report.generated_recommendations,
            'confidence': auto_report.overall_confidence,
            'requires_review': auto_report.requires_review,
            'triage': {
                'triage_level': m.get('triage_level'),
                'triage_score': m.get('triage_score'),
                'flagged': bool(m.get('triage_flagged')),
            },
            'reference_suggestions': m.get('reference_suggestions') or [],
            'online_references': m.get('online_references') or [],
        })
        
    except Exception as e:
        return JsonResponse({'error': f'Error generating report: {str(e)}'}, status=500)

@login_required
@user_passes_test(is_admin_or_radiologist)
def review_auto_report(request, report_id):
    """Review and approve/modify auto-generated report"""
    auto_report = get_object_or_404(AutoGeneratedReport, id=report_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and auto_report.study.facility != user.facility:
        messages.error(request, 'Permission denied')
        return redirect('ai_analysis:ai_dashboard')
    
    if request.method == 'POST':
        action = request.POST.get('action')
        
        if action == 'approve':
            auto_report.review_status = 'approved'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            # Create final report (when reports app is available)
            # auto_report.approve_and_create_report(user)
            
            messages.success(request, 'Auto-generated report approved successfully')
            
        elif action == 'modify':
            auto_report.generated_findings = request.POST.get('findings')
            auto_report.generated_impression = request.POST.get('impression')
            auto_report.generated_recommendations = request.POST.get('recommendations')
            auto_report.review_status = 'modified'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            messages.success(request, 'Auto-generated report modified and approved')
            
        elif action == 'reject':
            auto_report.review_status = 'rejected'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            messages.success(request, 'Auto-generated report rejected')
        
        return redirect('ai_analysis:ai_dashboard')
    
    # GET request - show review form
    context = {
        'auto_report': auto_report,
        'study': auto_report.study,
        'ai_analysis': auto_report.ai_analysis,
    }
    
    return render(request, 'ai_analysis/review_auto_report.html', context)

@login_required
@csrf_exempt
def api_ai_feedback(request, analysis_id):
    """Submit feedback on AI analysis"""
    analysis = get_object_or_404(AIAnalysis, id=analysis_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and analysis.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            feedback = AIFeedback.objects.create(
                ai_analysis=analysis,
                user=user,
                feedback_type=data.get('feedback_type'),
                rating=data.get('rating'),
                comments=data.get('comments', ''),
                incorrect_findings=data.get('incorrect_findings', []),
                missed_findings=data.get('missed_findings', []),
                suggestions=data.get('suggestions', '')
            )
            
            return JsonResponse({
                'success': True,
                'feedback_id': feedback.id,
                'message': 'Feedback submitted successfully'
            })
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

@login_required
@user_passes_test(is_admin_or_radiologist)
def model_management(request):
    """AI model management interface"""
    models = AIModel.objects.all().order_by('-created_at')
    
    # Search and filtering
    search_query = request.GET.get('search', '')
    if search_query:
        models = models.filter(
            Q(name__icontains=search_query) |
            Q(description__icontains=search_query) |
            Q(modality__icontains=search_query)
        )
    
    model_type_filter = request.GET.get('model_type', '')
    if model_type_filter:
        models = models.filter(model_type=model_type_filter)
    
    # Pagination
    paginator = Paginator(models, 20)
    page_number = request.GET.get('page')
    models_page = paginator.get_page(page_number)
    
    context = {
        'models': models_page,
        'search_query': search_query,
        'model_type_filter': model_type_filter,
        'model_types': AIModel.MODEL_TYPES,
    }
    
    return render(request, 'ai_analysis/model_management.html', context)

@login_required
@csrf_exempt
def api_realtime_analyses(request):
    """Get real-time AI analysis updates"""
    user = request.user
    
    # Get timestamp from request
    last_update = request.GET.get('last_update')
    
    try:
        if last_update:
            last_update_time = timezone.datetime.fromisoformat(last_update.replace('Z', '+00:00'))
        else:
            last_update_time = timezone.now() - timezone.timedelta(minutes=5)
    except:
        last_update_time = timezone.now() - timezone.timedelta(minutes=5)
    
    # Get analyses updated since last check.
    #
    # IMPORTANT:
    # `requested_at` alone is not enough because status transitions happen later
    # (started_at/completed_at). We treat those as "updates" too so dashboards
    # can reflect background processing accurately.
    base_q = Q(requested_at__gt=last_update_time) | Q(started_at__gt=last_update_time) | Q(completed_at__gt=last_update_time)
    if user.is_facility_user():
        analyses = (
            AIAnalysis.objects
            .filter(study__facility=user.facility)
            .filter(base_q)
            .select_related('study', 'ai_model', 'study__patient')
            .order_by('-requested_at')[:50]
        )
    else:
        analyses = (
            AIAnalysis.objects
            .filter(base_q)
            .select_related('study', 'ai_model', 'study__patient')
            .order_by('-requested_at')[:50]
        )
    
    clinician = is_admin_or_radiologist(user)
    analyses_data = []
    for analysis in analyses:
        item = {
            'id': analysis.id,
            'study_id': analysis.study.id,
            'accession_number': analysis.study.accession_number,
            'patient_name': analysis.study.patient.full_name,
            'ai_model': analysis.ai_model.name,
            'status': analysis.status,
            'priority': analysis.priority,
            'requested_at': analysis.requested_at.isoformat(),
            'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None,
        }
        if clinician:
            item['confidence_score'] = analysis.confidence_score
        analyses_data.append(item)
    
    # Provide lightweight stats for dashboard polling
    stats_qs = AIAnalysis.objects.all()
    if user.is_facility_user():
        stats_qs = stats_qs.filter(study__facility=user.facility)

    active_analyses = stats_qs.filter(status__in=['pending', 'processing']).count()
    completed_today = stats_qs.filter(completed_at__date=timezone.now().date(), status='completed').count()
    failed_today = stats_qs.filter(completed_at__date=timezone.now().date(), status='failed').count()

    return JsonResponse({
        'analyses': analyses_data,
        'timestamp': timezone.now().isoformat(),
        'count': len(analyses_data),
        'stats': {
            'active_analyses': active_analyses,
            'processing_queue': active_analyses,
            'completed_today': completed_today,
            'failed_today': failed_today,
        }
    })

@login_required
@user_passes_test(lambda u: u.is_admin())
def ai_reporting_dashboard(request):
    """Comprehensive AI reporting dashboard"""
    # Get date range from request
    end_date = timezone.now().date()
    start_date = end_date - timedelta(days=30)
    
    date_filter = request.GET.get('date_range', '30d')
    if date_filter == '7d':
        start_date = end_date - timedelta(days=7)
    elif date_filter == '90d':
        start_date = end_date - timedelta(days=90)
    elif date_filter == '1y':
        start_date = end_date - timedelta(days=365)
    
    # Overall statistics
    total_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date
    ).count()
    
    completed_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='completed'
    ).count()
    
    failed_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='failed'
    ).count()
    
    # Success rate
    success_rate = (completed_analyses / total_analyses * 100) if total_analyses > 0 else 0
    
    # Average processing time
    avg_processing_time = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='completed',
        processing_time__isnull=False
    ).aggregate(Avg('processing_time'))['processing_time__avg'] or 0
    
    # Model performance breakdown
    model_performance = []
    for model in AIModel.objects.filter(is_active=True):
        model_analyses = AIAnalysis.objects.filter(
            ai_model=model,
            requested_at__date__gte=start_date,
            requested_at__date__lte=end_date
        )
        
        total = model_analyses.count()
        completed = model_analyses.filter(status='completed').count()
        failed = model_analyses.filter(status='failed').count()
        avg_confidence = model_analyses.filter(
            status='completed',
            confidence_score__isnull=False
        ).aggregate(Avg('confidence_score'))['confidence_score__avg'] or 0
        
        model_performance.append({
            'model': model,
            'total_analyses': total,
            'completed': completed,
            'failed': failed,
            'success_rate': (completed / total * 100) if total > 0 else 0,
            'avg_confidence': round(avg_confidence, 3),
            'avg_processing_time': model.avg_processing_time or 0
        })
    
    # Auto-report statistics
    total_auto_reports = AutoGeneratedReport.objects.filter(
        generated_at__date__gte=start_date,
        generated_at__date__lte=end_date
    ).count()
    
    approved_reports = AutoGeneratedReport.objects.filter(
        generated_at__date__gte=start_date,
        generated_at__date__lte=end_date,
        review_status='approved'
    ).count()
    
    pending_reports = AutoGeneratedReport.objects.filter(
        review_status='pending'
    ).count()
    
    # Daily analysis trends
    daily_trends = []
    current_date = start_date
    while current_date <= end_date:
        daily_count = AIAnalysis.objects.filter(
            requested_at__date=current_date
        ).count()
        daily_trends.append({
            'date': current_date.isoformat(),
            'count': daily_count
        })
        current_date += timedelta(days=1)
    
    # Top performing models
    top_models = sorted(model_performance, key=lambda x: x['success_rate'], reverse=True)[:5]
    
    # Recent feedback
    recent_feedback = AIFeedback.objects.filter(
        created_at__date__gte=start_date
    ).select_related('ai_analysis__ai_model', 'user').order_by('-created_at')[:10]
    
    context = {
        'total_analyses': total_analyses,
        'completed_analyses': completed_analyses,
        'failed_analyses': failed_analyses,
        'success_rate': round(success_rate, 2),
        'avg_processing_time': round(avg_processing_time, 2),
        'model_performance': model_performance,
        'total_auto_reports': total_auto_reports,
        'approved_reports': approved_reports,
        'pending_reports': pending_reports,
        'daily_trends': daily_trends,
        'top_models': top_models,
        'recent_feedback': recent_feedback,
        'date_filter': date_filter,
        'start_date': start_date,
        'end_date': end_date,
    }
    
    return render(request, 'ai_analysis/reporting_dashboard.html', context)

@login_required
@user_passes_test(lambda u: u.is_admin())
def verify_ai_models(request):
    """Verify that all AI models are working correctly"""
    verification_results = []
    
    # Get all active AI models
    models = AIModel.objects.filter(is_active=True)
    
    for model in models:
        result = {
            'model': model,
            'status': 'unknown',
            'last_test': None,
            'test_results': {},
            'issues': []
        }
        
        # Check if model files exist
        if not os.path.exists(model.model_file_path):
            result['issues'].append('Model file not found')
            result['status'] = 'error'
        
        # Check recent analyses
        recent_analyses = AIAnalysis.objects.filter(
            ai_model=model,
            requested_at__gte=timezone.now() - timedelta(days=7)
        )
        
        if recent_analyses.exists():
            completed = recent_analyses.filter(status='completed').count()
            failed = recent_analyses.filter(status='failed').count()
            total = recent_analyses.count()
            
            success_rate = (completed / total * 100) if total > 0 else 0
            
            result['test_results'] = {
                'total_tests': total,
                'completed': completed,
                'failed': failed,
                'success_rate': round(success_rate, 2)
            }
            
            if success_rate >= 95:
                result['status'] = 'excellent'
            elif success_rate >= 85:
                result['status'] = 'good'
            elif success_rate >= 70:
                result['status'] = 'warning'
            else:
                result['status'] = 'error'
                result['issues'].append(f'Low success rate: {success_rate:.1f}%')
        else:
            result['status'] = 'untested'
            result['issues'].append('No recent test data available')
        
        # Check performance metrics
        latest_metric = model.performance_metrics.first()
        if latest_metric:
            result['last_test'] = latest_metric.evaluation_date
            if latest_metric.accuracy < 0.8:
                result['issues'].append(f'Low accuracy: {latest_metric.accuracy:.3f}')
        else:
            result['issues'].append('No performance metrics available')
        
        verification_results.append(result)
    
    # Overall system status
    statuses = [r['status'] for r in verification_results]
    if 'error' in statuses:
        overall_status = 'error'
    elif 'warning' in statuses:
        overall_status = 'warning'
    elif 'untested' in statuses:
        overall_status = 'warning'
    else:
        overall_status = 'good'
    
    context = {
        'verification_results': verification_results,
        'overall_status': overall_status,
        'total_models': len(verification_results),
        'error_count': statuses.count('error'),
        'warning_count': statuses.count('warning') + statuses.count('untested'),
        'good_count': statuses.count('good') + statuses.count('excellent'),
    }
    
    return render(request, 'ai_analysis/model_verification.html', context)

@login_required
@user_passes_test(lambda u: u.is_admin())
@csrf_exempt
def run_model_test(request, model_id):
    """Run a test on a specific AI model"""
    model = get_object_or_404(AIModel, id=model_id)
    
    if request.method == 'POST':
        try:
            # Get a test study for this modality
            test_study = Study.objects.filter(
                modality__code=model.modality
            ).first()
            
            if not test_study:
                return JsonResponse({
                    'success': False,
                    'error': f'No test studies available for modality {model.modality}'
                })
            
            # Create a test analysis
            test_analysis = AIAnalysis.objects.create(
                study=test_study,
                ai_model=model,
                priority='high',
                status='pending'
            )
            
            # Run the analysis in background
            run_ai_analysis.delay(test_analysis.id)
            
            return JsonResponse({
                'success': True,
                'analysis_id': test_analysis.id,
                'message': f'Test started for {model.name}'
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            })
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

@login_required
@csrf_exempt
def api_medical_references(request):
    """Get medical references for given keywords"""
    keywords = request.GET.get('keywords', '').split(',')
    keywords = [k.strip() for k in keywords if k.strip()]
    
    if not keywords:
        return JsonResponse({'references': []})
        
    references = _get_online_references(keywords)
    return JsonResponse({'references': references})
